## 1.背景介绍

## 1.1. 逻辑回归 Logistic Regression （对数几率回归 Logit Regression）

#### 名字

关于名字，有文献将Logistic Regression译为“逻辑回归”， 但中文“逻辑”与logitic 和 logit 的含义相去甚远，因此在《机器学习》中意译为“对数几率回归”，简称“对率回归”。

#### 线性回归

在介绍对数几率回归之前先介绍一下线性回归，线性回归的主要思想是通过历史数据拟合出一条直线，因变量与自变量是线性关系，对新的数据用这条直线进行预测。
线性回归的公式如下：

y=w0+w1x1+...+wnxn=wTx+b

#### 逻辑回归

对数几率回归是一种广义的线性回归分析模型，是一种预测分析。虽然它名字里带回归，但实际上对数几率回归是一种分类学习方法。它不是仅预测出“类别”， 而是可以得到近似概率预测，这对于许多需要利用概率辅助决策的任务很有用。普遍应用于预测一个实例是否属于一个特定类别的概率，比如一封email是垃圾邮件的概率是多少。 因变量可以是二分类的，也可以是多分类的。因为结果是概率的，除了分类外还可以做ranking model。LR的应用场景很多，如点击率预测（CTR）、天气预测、一些电商的购物搭配推荐、一些电商的搜索排序基线等。

对数几率函数是一种“Sigmoid”函数，呈现*S*型曲线，它将z值转化为一个接近0或1的 y值。
对数几率回归公式如下：

y=g(z)=11+e−z, z=wTx+b，

其中，y=11+e−x 被称作**Sigmoid**函数。

Logistic Regression算法是将线性函数的结果映射到了Sigmoid函数中，即y=11+e(wTx+b)。

#### Sigmoid函数

下图绘制了Sigmoid函数形状，如图所示，sigmoid函数输出值范围在（0，1）之间，即代表了数据属于某一类别的概率，0.5是作为判别的临界值。

In [3]:

```
# Sigmoid曲线:
import matplotlib.pyplot as plt
import numpy as np


def Sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

x = np.arange(-10, 10, 0.1)
h = Sigmoid(x)  # Sigmoid函数
plt.plot(x, h)
plt.axvline(0.0, color='k')
plt.axhline(y=0.5, ls='dotted', color='k')
plt.yticks([0.0,  0.5, 1.0])  # y axis label
plt.title(r'Sigmoid函数曲线', fontsize = 15)
plt.text(5,0.8,r'$y = \frac{1}{1+e^{-z}}$', fontsize = 18)
plt.show()
/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  'Matplotlib is building the font cache using fc-list. '
```

![img](http://7xrkee.com1.z0.glb.clouddn.com/rt_upload/1BAFE0911F544E378E7DBBF95F1BD965/1525226712275_330.png)

## 1.2.IRIS数据集介绍

Iris也称鸢尾花卉数据集,是常用的分类实验数据集，由R.A. Fisher于1936年收集整理的。其中包含3种植物种类，分别是**山鸢尾（setosa）变色鸢尾（versicolor）和维吉尼亚鸢尾（virginica）**，每类50个样本，共150个样本。

该数据集包含4个特征变量，1个类别变量。iris每个样本都包含了4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，以及1个类别变量（label）。我们需要建立一个分类器，分类器可以通过这4个特征来预测鸢尾花卉种类是属于山鸢尾，变色鸢尾还是维吉尼亚鸢尾。其中有一个类别是线性可分的，其余两个类别线性不可分，这在最后的分类结果绘制图中可观察到。

| 变量名       | 变量解释           | 数据类型    |
| :----------- | :----------------- | :---------- |
| sepal_length | 花萼长度（单位cm   | numeric     |
| sepal_width  | 花萼宽度（单位cm） | numeric     |
| petal_length | 花瓣长度（单位cm） | numeric     |
| petal_width  | 花瓣宽度（单位cm） | numeric     |
| species      | 种类               | categorical |

In [4]:

```
# 导入所需要的包
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.plotly as py
import plotly.graph_objs as go
from sklearn.decomposition import PCA

from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
```

### 1.2.1. 数据集预览

In [5]:

```
iris_path = '/home/kesci/input/iris/iris.csv'
data = pd.read_csv(iris_path)
```

In [6]:

```
data.head()
```

Out[6]:

|      | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species |
| :--- | -----------: | ----------: | -----------: | ----------: | ------: |
| 0    |          5.1 |         3.5 |          1.4 |         0.2 |  setosa |
| 1    |          4.9 |         3.0 |          1.4 |         0.2 |  setosa |
| 2    |          4.7 |         3.2 |          1.3 |         0.2 |  setosa |
| 3    |          4.6 |         3.1 |          1.5 |         0.2 |  setosa |
| 4    |          5.0 |         3.6 |          1.4 |         0.2 |  setosa |

#### 1.2.2. 鸢尾花三类品种数量的饼图

In [7]:

```
labels = data.groupby('Species').size().index
values = data.groupby('Species').size()
trace = go.Pie(labels=labels, values=values)
layout = go.Layout(width=350, height=350)
fig = go.Figure(data=[trace], layout=layout)
iplot(fig)
```

In [8]:

```
# Feature Plot
groups = data.groupby(by = "Species")
means, sds = groups.mean(), groups.std()
means.plot(yerr = sds, kind = 'bar', figsize = (9, 5), table = True)
plt.show()
```

![img](http://7xrkee.com1.z0.glb.clouddn.com/rt_upload/1BAFE0911F544E378E7DBBF95F1BD965/1525226877008_302.png)

#### 1.2.3. 绘制数据集的特征散点图

特征对两两之间的**相关性散点图**：
如图所示，特征散点图成对角分布，4个特征两两组合（任意两个特征作为x轴，y轴），不同品种的花用不同颜色标注：setosa（橙色），versicolor（绿色），virginica（粉色）。共有12种组合，其实只有6种，因为另外6种与之对称。

In [9]:

```
col_map = {'setosa': 'orange', 'versicolor': 'green', 'virginica': 'pink'}
pd.tools.plotting.scatter_matrix(data.loc[:, 'Sepal.Length':'Petal.Width']
, diagonal = 'kde', color = [col_map[lb] for lb in data['Species']], s = 75, figsize = (11, 6))
plt.show()
/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:3: FutureWarning:

'pandas.tools.plotting.scatter_matrix' is deprecated, import 'pandas.plotting.scatter_matrix' instead.
```

![img](http://7xrkee.com1.z0.glb.clouddn.com/rt_upload/1BAFE0911F544E378E7DBBF95F1BD965/1525226880468_558.png)

## 2. Getting Started

### 2.1. 导入鸢尾花数据集矩阵

在这篇入门教程中，暂且不进行数据转换至numpy矩阵的指导，因为scikit库中已经内置了矩阵形式的iris数据集，我们可以直接导入使用。
如果想了解 如何将原始数据转变成机器学习算法可学习的numpy数据集，以及 数据预处理 和 降维 的小伙伴，可以关注下一篇教程 **[用逻辑回归实现鸢尾花数据集分类（2）](http://www.kesci.com/apps/home/project/5ae6826c0739c42faa1db074)**。

In [10]:

```
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
```

In [11]:

```
iris = load_iris()
```

In [12]:

```
# data对应了样本的4个特征，共150个样本，即150行x4列的矩阵
print("Iris Dataset contains %s samples in total，%s features."%(iris.data.shape[0], iris.data.shape[1]))
Iris Dataset contains 150 samples in total，4 features.
```

In [13]:

```
iris.data[:5]
```

Out[13]:

```
array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
       [5. , 3.6, 1.4, 0.2]])
```

target代表150个样本对应的类别label,即150行x1列的矩阵

样本的类别label含义

| Class Label | Meaning                   |
| :---------- | :------------------------ |
| 0           | 山鸢尾（setosa）          |
| 1           | 变色鸢尾（versicolor））  |
| 2           | 维吉尼亚鸢尾（virginica） |

In [14]:

```
print("Labels' shape %s." %(iris.target.shape))
Labels' shape 150.
```

In [15]:

```
iris.target
```

Out[15]:

```
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
```

### 2.2 创建训练集与测试集

在这里我们先前取两列数据（即特征花萼长度与宽度）进行对数几率回归的分类。这个例子借鉴于[此](http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html)。
用train_test_split函数将原始数据集按7:3的比例分成训练集与测试集

In [16]:

```
from sklearn.model_selection import train_test_split

X = iris.data[:, :2]             # 取前两列数据
Y = iris.target

x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)
```

In [17]:

```
x_train.shape,y_train.shape,x_test.shape, y_test.shape
```

Out[17]:

```
((105, 2), (105,), (45, 2), (45,))
```

#### 绘制前两列数据的散点图

In [18]:

```
# 画出训练集数据点
trace = go.Scatter(x = X[:,0], y = X[:,1], mode = 'markers', 
                    marker = dict(color = np.random.randn(150),size = 10, colorscale='Viridis',showscale=False))
layout = go.Layout(title = '训练点', xaxis=dict(title='花萼长度 Sepal length', showgrid=False),
                    yaxis=dict(title='花萼宽度 Sepal width',showgrid=False),
                    width = 700, height = 380)
fig = go.Figure(data=[trace], layout=layout)
```

In [19]:

```
iplot(fig)
```

### 2.3. 模型搭建与分类器训练

1. 导入模型，调用逻辑回归

   LogisticRegression()

   函数。

   - penalty: 正则化选择参数（惩罚项的种类），默认方式为L2正则化
   - C: 正则项系数的倒数
   - solver: 对于多分类任务， 使用‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ 来解决多项式loss
   - multi_class: 默认值‘ovr’适用于二分类问题，对于多分类问题，用‘multinomial’在全局的概率分布上最小化损失

2. 训练LogisticRegression分类器

   - 调用fit(x,y)的方法来训练模型，其中x为数据的属性，y为所属类型。

3. 利用训练得到的模型对数据集进行预测 predict()，返回预测结果。

> Tips:
> 可以通过点击cell中的+来“添加代码片段功能”来直接导入需要的代码

In [20]:

```
from sklearn.linear_model import LogisticRegression

# lr = LogisticRegression(C = 1e5) # C: Inverse of regularization strength
lr = LogisticRegression(penalty='l2',solver='newton-cg',multi_class='multinomial')
lr.fit(x_train,y_train)
```

Out[20]:

```
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='multinomial',
          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',
          tol=0.0001, verbose=0, warm_start=False)
```

### 2.4. 模型评估

In [21]:

```
print("Logistic Regression模型训练集的准确率：%.3f" %lr.score(x_train, y_train))
print("Logistic Regression模型测试集的准确率：%.3f" %lr.score(x_test, y_test))
Logistic Regression模型训练集的准确率：0.829
Logistic Regression模型测试集的准确率：0.822
```

LogisticRegression分类器正确率分析

In [22]:

```
from sklearn import metrics
y_hat = lr.predict(x_test)
accuracy = metrics.accuracy_score(y_test, y_hat) #错误率，也就是np.average(y_test==y_pred)
print("Logistic Regression模型正确率：%.3f" %accuracy)
Logistic Regression模型正确率：0.822
```

In [23]:

```
target_names = ['setosa', 'versicolor', 'virginica']
print(metrics.classification_report(y_test, y_hat, target_names = target_names))
             precision    recall  f1-score   support

     setosa       1.00      1.00      1.00        16
 versicolor       0.81      0.72      0.76        18
  virginica       0.62      0.73      0.67        11

avg / total       0.83      0.82      0.82        45
```

### 2.5. 可视化分类结果

#### 绘制图像

下图会绘制逻辑回归分类器在鸢尾花数据集上的决策边界，不同类别的数据点用不同颜色标注。
为了能可视化分类效果，我们会画出决策边界（decision boundry）。

1.确定坐标轴范围，x,y轴各表示一个特征

```
- 先取二维数组的第一列特征（花萼长度）的最大最小值和步长h = .02生成数组，  
- 再取二维数组的第二列特征（花萼宽度）的最大最小值和步长h = .02生成数组，  
```

最后由meshgrid()函数在网格[x_min, x_max] x [y_min, y_max] 中绘制出。生成两个网格矩阵x1, x2

In [24]:

```
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x1_min, x1_max = X[:, 0].min() - .5, X[:, 0].max() + .5 # 第0列的范围
x2_min, x2_max = X[:, 1].min() - .5, X[:, 1].max() + .5 # 第1列的范围
h = .02
x1, x2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h)) # 生成网格采样点
```

In [25]:

```
grid_test = np.stack((x1.flat, x2.flat), axis=1)  # 测试点
grid_hat = lr.predict(grid_test)                  # 预测分类值
# grid_hat = lr.predict(np.c_[x1.ravel(), x2.ravel()])
grid_hat = grid_hat.reshape(x1.shape)             # 使之与输入的形状相同
```

In [55]:

```
plt.figure(1, figsize=(6, 5))
# 预测值的显示, 输出为三个颜色区块，分布表示分类的三类区域
plt.pcolormesh(x1, x2, grid_hat,cmap=plt.cm.Paired) 

# plt.scatter(X[:, 0], X[:, 1], c=Y,edgecolors='k', cmap=plt.cm.Paired)
plt.scatter(X[:50, 0], X[:50, 1], marker = '*', edgecolors='red', label='setosa')
plt.scatter(X[50:100, 0], X[50:100, 1], marker = '+', edgecolors='k', label='versicolor')
plt.scatter(X[100:150, 0], X[100:150, 1], marker = 'o', edgecolors='k', label='virginica')
plt.xlabel('花萼长度-Sepal length')
plt.ylabel('花萼宽度-Sepal width')
plt.legend(loc = 2)

plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(), x2.max())
plt.title("Logistic Regression 鸢尾花分类结果", fontsize = 15)
plt.xticks(())
plt.yticks(())
plt.grid()

plt.show()
```





### 二

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import plotly.plotly as py
import plotly.graph_objs as go


from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
```

## 1. 加载鸢尾花数据集

In [2]:

```
iris_path = '/home/kesci/input/iris/iris.csv'
iris = pd.read_csv(iris_path)
```

In [3]:

```
iris.head()
```

Out[3]:

|      | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species |
| :--- | -----------: | ----------: | -----------: | ----------: | ------: |
| 0    |          5.1 |         3.5 |          1.4 |         0.2 |  setosa |
| 1    |          4.9 |         3.0 |          1.4 |         0.2 |  setosa |
| 2    |          4.7 |         3.2 |          1.3 |         0.2 |  setosa |
| 3    |          4.6 |         3.1 |          1.5 |         0.2 |  setosa |
| 4    |          5.0 |         3.6 |          1.4 |         0.2 |  setosa |

## 1.1. 从csv文件数据到Numpy数据的构造过程

Step 1：构造映射函数iris_type。因为实际数据中，label并不都是便于学习分类的数字型，而是string类型。
Step 2：对于文本类的label, 将label列的所有内容都转变成映射函数的输出，存成新的dataframe
Step 3：将Step2的结果转换成numpy矩阵
Step 4：划分训练集与测试集

In [4]:

```
# S1:
# 映射函数iris_type: 将string的label映射至数字label
# s: 品种的名字
def iris_type(s):
    class_label = {'setosa':0, 'versicolor':1, 'virginica':2}
    return class_label[s]
```

In [5]:

```
# Step 2: 将第4列内容映射至iris_type函数定义的内容,查看效果
new_iris = pd.io.parsers.read_csv(iris_path, converters = {4:iris_type})
new_iris.head()
```

Out[5]:

|      | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species |
| :--- | -----------: | ----------: | -----------: | ----------: | ------: |
| 0    |          5.1 |         3.5 |          1.4 |         0.2 |       0 |
| 1    |          4.9 |         3.0 |          1.4 |         0.2 |       0 |
| 2    |          4.7 |         3.2 |          1.3 |         0.2 |       0 |
| 3    |          4.6 |         3.1 |          1.5 |         0.2 |       0 |
| 4    |          5.0 |         3.6 |          1.4 |         0.2 |       0 |

In [6]:

```
# Step 3: 将new_iris解析至numpy array
data = np.array(new_iris)  # 或者直接new_iris.values,结果是一样的
data[:10,:]        # 查看前10行的数据
```

Out[6]:

```
array([[5.1, 3.5, 1.4, 0.2, 0. ],
       [4.9, 3. , 1.4, 0.2, 0. ],
       [4.7, 3.2, 1.3, 0.2, 0. ],
       [4.6, 3.1, 1.5, 0.2, 0. ],
       [5. , 3.6, 1.4, 0.2, 0. ],
       [5.4, 3.9, 1.7, 0.4, 0. ],
       [4.6, 3.4, 1.4, 0.3, 0. ],
       [5. , 3.4, 1.5, 0.2, 0. ],
       [4.4, 2.9, 1.4, 0.2, 0. ],
       [4.9, 3.1, 1.5, 0.1, 0. ]])
```

In [7]:

```
# Step 4:将原始数据集划分成训练集与测试集

# 用np.split按列（axis=1）进行分割
# (4,):分割位置，前4列作为x的数据，第4列之后都是y的数据
x,y = np.split(data, (4,), axis = 1)  
# X = x[:,0:2] # 取前两列特征
# 用train_test_split将数据按照7：3的比例分割训练集与测试集，
# 随机种子设为1（每次得到一样的随机数），设为0或不设（每次随机数都不同）
x_train, x_test, y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 0)
```

## 2. 模型的搭建与训练

- Pipeline(steps)

  利用sklearn提供的管道机制

  Pipeline

  来实现对全部步骤的流式化封装与管理。

  - 第一个环节：可以先进行 *数据标准化 StandardScaler()*
  - 中间环节：可以加上 *PCA降维*处理 取2个重要特征
  - 最终环节：逻辑回归分类器

In [8]:

```
pipe_LR = Pipeline([
                    ('sc', StandardScaler()),
                    ('pca', PCA(n_components = 2)),
                    ('clf_lr', LogisticRegression(random_state=1))
                    ])
# 开始训练
pipe_LR.fit(x_train, y_train.ravel())
```

Out[8]:

```
Pipeline(memory=None,
     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf_lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))])
```

## 3. 分类器评估

### 3.1. 准确率

In [9]:

```
print("训练集准确率: %0.2f" %pipe_LR.score(x_train, y_train))
训练集准确率: 0.87
```

In [10]:

```
print("测试集准确率: %0.2f" %pipe_LR.score(x_test, y_test))
测试集准确率: 0.87
```

In [11]:

```
y_hat = pipe_LR.predict(x_test)
accuracy = metrics.accuracy_score(y_test, y_hat)
print("逻辑回归分类器的准确率：%0.2f" % accuracy)
逻辑回归分类器的准确率：0.87
```

### 3.2.分类器的分类报告总结

- 精确度（Precision）
- 召回率(Recall)
- F1 Score

In [12]:

```
target_names = ['setosa', 'versicolor', 'virginica']
print(metrics.classification_report(y_test, y_hat, target_names = target_names))
             precision    recall  f1-score   support

     setosa       1.00      1.00      1.00        16
 versicolor       1.00      0.67      0.80        18
  virginica       0.65      1.00      0.79        11

avg / total       0.91      0.87      0.87        45
```

## 4.拓展

### 4.1. 用交叉验证（Cross Validation）来验证分类器性能

[交叉验证](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)常用于防止模型过于复杂而造成过拟合，同时也称为循环估计。基本思想是将原始数据分成K组（一般是平均分组），每个子集数据分别做一次验证集或测试集，其余的K-1个子集作为训练集。这样就会得到K个模型，取这K个模型的分类准确率的平均数作为分类器的性能指标更具说服力。

比如说在这里我们使用的是5折交叉验证(5-fold cross validation)，即数据集被分成了5份，轮流将其中4份作为训练数据集，剩余1份作为测试集，进行试验。每次试验都会得出相应的正确率，将5次试验得出的相应正确率的平均值作为分类器的准确率的估计。同样的，K也可以取10，20等。

In [13]:

```
iris_data = x
iris_target = y
```

In [14]:

```
from sklearn.model_selection import cross_val_score

scores = cross_val_score(pipe_LR, iris_data, iris_target.ravel(), cv = 5,scoring='f1_macro') # ravel() 将y shape转变成（n_samples,）
print("5折交叉验证:\n逻辑回归分类器的准确率：%.2f 误差范围：(+/- %.2f)"%(scores.mean(), scores.std()*2))
5折交叉验证:
逻辑回归分类器的准确率：0.86 误差范围：(+/- 0.17)
```

### 4.2. 简单的网格搜索（Simple Grid Search）

通常数据集被分割成训练集与测试集，即测试集除了用来调参，也被用来评估模型的好坏。这样做的结果会导致模型的最终评分结果会比实际要好。因为测试集在调参过程中，被用在了模型上，而训练模型的目的是为了用在unseen data[2]。这对应的解决方法就是对训练集再进行一次分割：分成**训练集**与**验证集**。如下图所示，原始数据被分割了3份：训练集、验证集、测试集。其中**训练集被用来模型训练，验证集用来调整参数，测试集用来评估模型好坏**。

![Image Name](https://cdn.kesci.com/user_upload/image/1527599437909_45128.png?imageView2/0/w/640/h/640)

In [15]:

```
X_trainval, X_test, y_trainval, y_test = train_test_split(
    iris_data, iris_target, random_state=0)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, random_state=1)
print("训练集大小:{} 验证集大小:{} 测试集大小:{}".format(
        X_train.shape[0],
        X_val.shape[0],
        X_test.shape[0]))
训练集大小:84 验证集大小:28 测试集大小:38
```

In [16]:

```
best_score = 0.0
for penalty in ['l1','l2']:
    for C in [0.01,0.1, 1, 10, 100]:
        lr_clf = LogisticRegression(C = C, penalty = penalty)
        lr_clf.fit(X_train, y_train.ravel())          # 训练
        score = lr_clf.score(X_val, y_val.ravel())    # 调参
        if score > best_score:                # 找到最好score下的参数
            best_score = score
            best_parameters = {'penalty':penalty,'C':C}
lr = LogisticRegression(**best_parameters)    #使用最佳参数，构建新的模型
lr.fit(X_trainval,y_trainval.ravel())         #使用训练集和验证集进行训练,因为数据更多效果更好
test_score = lr.score(X_test,y_test.ravel())  # evaluation模型评估
print("验证集 best score: %.2f"%(best_score))
print("最好的参数:{}".format(best_parameters))
print("测试集 best score: %.2f" %(test_score))
验证集 best score: 0.96
最好的参数:{'C': 1, 'penalty': 'l1'}
测试集 best score: 0.89
```

但是，使用这种grid search方法的结果受初始数据的划分结果有很大影响，为了解决这个问题，下面教大家如何结合网格搜索与交叉验证的方式来减少这种偶然性。

### 4.1. 网格搜索验证 Grid Search Cross-Validation

[网格搜索验证](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)（Grid Search Cross-Validation）用于选择模型的最优超参值。Grid Search的作用是用来**调参**。

In [17]:

```
print("Start GridSearch...")
from sklearn.model_selection import GridSearchCV

param_range = [0.01,0.1, 1, 10, 100]     # 参数集合 

param_grid_lr= {'C': param_range,        # 正则化系数λ的倒数,数值越小，正则化越强
                'penalty': ['l1','l2']}  # 对参数的惩罚项(约束),增强泛化能力，防止overfit
# 创建 grid search实例            
clf = GridSearchCV(estimator = LogisticRegression(random_state=0), # 模型
                    param_grid = param_grid_lr,
                    scoring = 'accuracy',
                    cv = 10)                         # 10折交叉验证
Start GridSearch...
```

In [18]:

```
# fit grid search
best_model = clf.fit(X_trainval,y_trainval.ravel())

# 查看效果最好的超参数
print("最好模型的超参数：")
print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])
print('Best C:', best_model.best_estimator_.get_params()['C'])
print('逻辑回归模型best score:%.2f' % best_model.best_score_)
print("测试集准确率: %0.2f" %best_model.score(X_test, y_test))
最好模型的超参数：
Best Penalty: l1
Best C: 10
逻辑回归模型best score:0.98
测试集准确率: 0.97
```

### 模型评估总结：

**直接用逻辑回归得到的分数大约是86%，经过网格搜索优化后，可以在测试集上得到97%的准确率**。网格搜索交叉验证的好处不言而喻。

由于网格搜索是在参数列表中进行**穷举搜索**，对不同参数分布进行训练，从而找到最优的参数。因而，**网格搜索也有它的缺点**，就是耗时，参数越多越耗时。