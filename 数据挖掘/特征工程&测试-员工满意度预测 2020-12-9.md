In [1]:

```
import pandas as pd  # 数据表处理
import numpy as np  # 数值运算，线性代数
import matplotlib.pyplot as plt  # 数据可视化绘图
import seaborn as sns  # 数据可视化绘图
from scipy import stats  # 分布
%matplotlib inline

from collections import Counter
from sklearn.model_selection import train_test_split,cross_val_score  # 划分训练集测试集、交叉验证
from sklearn.preprocessing import OneHotEncoder,LabelEncoder,OrdinalEncoder, MinMaxScaler, MaxAbsScaler  # 特征编码
from sklearn.decomposition import PCA  # 数据降维
import time
import pickle as pkl  # 序列化
from sklearn.metrics import mean_squared_error, make_scorer  # 评估指标
from pandas import DataFrame, Series 
from sklearn.model_selection import KFold, StratifiedKFold  # 交叉验证
from sklearn.model_selection import GridSearchCV  # 网格搜索
from sklearn.ensemble import RandomForestRegressor  # 随机森林回归模型
```

In [2]:

```
train_data = pd.read_csv('./Employee_Satisfaction/训练集.csv')
test_data = pd.read_csv('./Employee_Satisfaction/测试集.csv')
```

In [3]:

```
import warnings

warnings.filterwarnings("ignore")
```

In [4]:

```
train_data.index = train_data.id  # 不用鸟 
test_data.index = test_data.id  # 不用鸟 

x_train = train_data.drop(['satisfaction_level', 'id'], axis=1)  # 训练集数据去掉 'satisfaction_level', 'id' 这两列
y_train = train_data.satisfaction_level  # 标签数据

x_test = test_data.drop(['id'], axis=1)

def _encode(data):
    result = pd.DataFrame.copy(data, deep=True)
    
    # 下面这行代码的灵感，来自数据探索性分析中对 package 特征的可视化分析
    result["package"] = result["package"].apply(lambda x:{"a":"0", "c":"0", "e":"0", "b":"1", "d":"1"}[x]) # 降低了0.0002个mse
    
    # 类别编码 -> 把类似 'a', 'b' 这种字符编码为 0, 1 这种数值，因为模型只能接收数值
    division_le = LabelEncoder()
    package_le = LabelEncoder()
    salary_oe = LabelEncoder()
    result.division = division_le.fit_transform(result['division'])
    result.salary = salary_oe.fit_transform(result['salary'])
    result.package = package_le.fit_transform(result['package'])

    # 对两个连续值的特征进行无量纲化
    for col in ['last_evaluation', 'average_monthly_hours']:
        maxAbsEnc = MaxAbsScaler()
        result[col] = maxAbsEnc.fit_transform(result[col].values.reshape(-1,1))

    # 独热编码
    result = pd.get_dummies(result, columns=["number_project", "time_spend_company", "Work_accident",
                                             "package", "promotion_last_5years", "division", "salary"])
    return result

# 对加载进来的数据集进行预处理
x_train_temp = _encode(x_train)
x_test_temp = _encode(x_test)
```

In [5]:

```
x_train_temp.shape
```

Out[5]:

```
(11999, 35)
```

In [6]:

```
# 用于记录本地时间
def get_local_time():
    return time.strftime("%m%d%H%M", time.localtime())

x_train = x_train_temp
x_test_id = x_test_temp.index
```

In [7]:

```
# 切分训练集和测试集
x_tr, x_va, y_tr, y_va = train_test_split(x_train, y_train, test_size = 0.2, random_state=30)
```

## 根据数据认知进行特征创建[¶](#根据数据认知进行特征创建)

### package 特征预处理[¶](#package-特征预处理)

In [8]:

```
# 初始化随机森林回归模型
random_forest_reg = RandomForestRegressor()
# 从里面搜索出让模型有最优表现的参数
param_grid = {'n_estimators': [100,150,200,250]}
# 定义网格搜索--从里面搜索出让模型有最优表现的参数
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)
# 开始搜
grid_search_cv.fit(x_tr, y_tr)


print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va), y_va)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.2min finished
测试集上的MSE: 0.031092557424066806
```

In [9]:

```
# 查看最优的参数
grid_search_cv.best_params_
```

Out[9]:

```
{'n_estimators': 200}
```

记录：

| 处理          | MSE                  |
| ------------- | -------------------- |
| 原始的one-hot | 0.03224754452495834  |
| 处理package   | 0.031092557424066806 |

<font size=6 color="red">从这开始，后面很多都是复制粘贴！！！</font>

<br>

In [11]:

```
# 用全部数据跑一下
random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)
grid_search_cv.fit(x_train, y_train)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_train), y_train)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.7min finished
测试集上的MSE: 0.004117739423642565
```

In [37]:

```
# 特征重要性可视化----知道是可视化特征的重要性就行了
feat_labels = x_train.columns[1:]
importances = grid_search_cv.best_estimator_.feature_importances_
indices = np.argsort(importances)[::-1]

plot_x = list()
plot_y = list()

# 获取特征名字
for f in range(x_train.shape[1]):
    try:
        plot_x.append(feat_labels[indices[f]])
        plot_y.append(importances[indices[f]])
#         print("%2d) %-*s %f" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))
    except:
        pass

figure = plt.figure(figsize=(10, 10))
plt.xticks(rotation=90)
sns.barplot(plot_x, plot_y)
```

Out[37]:

```
<AxesSubplot:>
```

![img](data:image/png;base64,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 ) 

从特征重要性可视化得知：`number_project` 特征的区分度很高，尝试对 `number_project` 的7号特征进行惩戒；<br>
**<font color="red">此举印证了数据探索时的灵感</font>**，此时去查看 `number_project` 中各个类别对应的标签的值的分布、统计量等信息

In [28]:

```
# 提交数据

result = pd.DataFrame({'id': x_test_temp.index, 'satisfaction_level': grid_search_cv.predict(x_test_temp)})
time_stamp = get_local_time()
result.to_csv('./处理package_randomforset-%s.csv'%time_stamp, index=False)
```

**加入了对package的预处理特征之后 测试集的MSE减小到 0.02936279**

## 笛卡尔积特征创建（离散值） + 特征选择[¶](#笛卡尔积特征创建（离散值）-+-特征选择)

显然这个数据集中离散值的占有重要地位

In [42]:

```
# 笛卡尔积特征创建----二阶笛卡尔积
def cartesian_product_feature_crosses(df, feature1_name, feature2_name):
    feature1_df = pd.get_dummies(df[feature1_name], prefix=feature1_name)
    feature1_columns = feature1_df.columns
 
    feature2_df = pd.get_dummies(df[feature2_name], prefix=feature2_name)
    feature2_columns = feature2_df.columns
 
    combine_df = pd.concat([feature1_df, feature2_df], axis=1)
 
    crosses_feature_columns = []
    for feature1 in feature1_columns:
        for feature2 in feature2_columns:
            crosses_feature = '{}&{}'.format(feature1, feature2)
            crosses_feature_columns.append(crosses_feature)
 
            combine_df[crosses_feature] = combine_df[feature1] * combine_df[feature2]
 
    combine_df = combine_df.loc[:, crosses_feature_columns]
    return combine_df
 
# cartesian_product_feature_crosses(df, 'color', 'light')
```

In [87]:

```
# 复制粘贴
train_data.index = train_data.id
test_data.index = test_data.id

x_train = train_data.drop(['satisfaction_level', 'id'], axis=1)
y_train = train_data.satisfaction_level

x_test = test_data.drop(['id'], axis=1)

def _encode(data):
    result = pd.DataFrame.copy(data, deep=True)
    # 先把非数值离散数据转化成数值型离散数据
    result["package"] = result["package"].apply(lambda x:{"a":"0", "c":"0", "e":"0",
                                                          "b":"1", "d":"1"}[x]) # 降低了0.0002个mse
    division_le = LabelEncoder()
    package_le = LabelEncoder()
    salary_oe = LabelEncoder()

    result.division = division_le.fit_transform(result['division'])
    result.salary = salary_oe.fit_transform(result['salary'])
    result.package = package_le.fit_transform(result['package'])

    for col in ['last_evaluation', 'average_monthly_hours']:
        maxAbsEnc = MaxAbsScaler()
        result[col] = maxAbsEnc.fit_transform(result[col].values.reshape(-1,1))

    # 独热编码
    cat_col_here = ["number_project", "time_spend_company", "Work_accident",
                       "package", "promotion_last_5years", "division", "salary"]
    
#     print(result.columns)
    # 进行二阶特征组合
    for i in range(len(cat_col_here)):
        for j in range(i+1, len(cat_col_here)):
            print(cat_col_here[i], cat_col_here[j])
            temp_result = cartesian_product_feature_crosses(result, cat_col_here[i], cat_col_here[j])
            result = pd.concat([result, temp_result], axis=1, join='inner')
    result = pd.get_dummies(result, columns=["number_project", "time_spend_company", "Work_accident",
                                             "package", "promotion_last_5years", "division", "salary"])
    return result

# x_train_temp = _encode(x_train)
# x_test_temp = _encode(x_test)
x_train = _encode(x_train)
```

In [159]:

```
x_test = _encode(x_test)
```

In [160]:

```
x_test.shape
```

Out[160]:

```
(3000, 469)
```

In [109]:

```
# 为了从处理过后的数据表中，取出新构造的特征所有的特征名，先得到原有的特征名
origin_col = x_train.columns[436:].tolist()
origin_col = [*origin_col, *x_train.columns[:2]]
```

In [125]:

```
x_train[origin_col]
```

Out[125]:

|       | number_project_2 | number_project_3 | number_project_4 | number_project_5 | number_project_6 | number_project_7 | time_spend_company_2 | time_spend_company_3 | time_spend_company_4 | time_spend_company_5 | ...  | division_5 | division_6 | division_7 | division_8 | division_9 | salary_0 | salary_1 | salary_2 | last_evaluation | average_monthly_hours |
| ----- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | -------------------- | -------------------- | -------------------- | -------------------- | ---- | ---------- | ---------- | ---------- | ---------- | ---------- | -------- | -------- | -------- | --------------- | --------------------- |
| id    |                  |                  |                  |                  |                  |                  |                      |                      |                      |                      |      |            |            |            |            |            |          |          |          |                 |                       |
| 13697 | 0                | 1                | 0                | 0                | 0                | 0                | 1                    | 0                    | 0                    | 0                    | ...  | 0          | 0          | 0          | 0          | 0          | 0        | 0        | 1        | 0.99            | 0.515442              |
| 1142  | 0                | 0                | 0                | 1                | 0                | 0                | 0                    | 0                    | 0                    | 0                    | ...  | 1          | 0          | 0          | 0          | 0          | 0        | 1        | 0        | 1.00            | 0.722494              |
| 7954  | 0                | 0                | 0                | 1                | 0                | 0                | 1                    | 0                    | 0                    | 0                    | ...  | 0          | 0          | 1          | 0          | 0          | 0        | 0        | 1        | 0.91            | 0.636997              |
| 2225  | 0                | 1                | 0                | 0                | 0                | 0                | 0                    | 1                    | 0                    | 0                    | ...  | 0          | 0          | 1          | 0          | 0          | 0        | 1        | 0        | 0.51            | 0.750982              |
| 9753  | 0                | 1                | 0                | 0                | 0                | 0                | 1                    | 0                    | 0                    | 0                    | ...  | 0          | 0          | 0          | 0          | 1          | 0        | 1        | 0        | 0.89            | 0.702341              |
| ...   | ...              | ...              | ...              | ...              | ...              | ...              | ...                  | ...                  | ...                  | ...                  | ...  | ...        | ...        | ...        | ...        | ...        | ...      | ...      | ...      | ...             | ...                   |
| 11971 | 0                | 0                | 1                | 0                | 0                | 0                | 0                    | 1                    | 0                    | 0                    | ...  | 0          | 0          | 0          | 0          | 0          | 1        | 0        | 0        | 0.89            | 0.480502              |
| 14966 | 0                | 0                | 0                | 1                | 0                | 0                | 0                    | 0                    | 0                    | 1                    | ...  | 0          | 0          | 1          | 0          | 0          | 0        | 1        | 0        | 0.81            | 0.706716              |
| 7491  | 0                | 0                | 1                | 0                | 0                | 0                | 0                    | 0                    | 1                    | 0                    | ...  | 0          | 0          | 1          | 0          | 0          | 0        | 0        | 1        | 0.60            | 0.435502              |
| 12680 | 0                | 0                | 0                | 0                | 1                | 0                | 0                    | 0                    | 1                    | 0                    | ...  | 0          | 0          | 0          | 0          | 0          | 0        | 1        | 0        | 0.84            | 0.857654              |
| 9056  | 1                | 0                | 0                | 0                | 0                | 0                | 0                    | 1                    | 0                    | 0                    | ...  | 0          | 0          | 0          | 1          | 0          | 0        | 1        | 0        | 0.84            | 0.611926              |

11999 rows × 35 columns

In [112]:

```
filter_columns = [col for col in x_train.columns if col not in origin_col] # 笛卡尔积创建出来的特征
```

In [118]:

```
# 特征选择----f检验
from sklearn.feature_selection import SelectKBest, f_regression
from scipy.stats import pearsonr

# 根据 f 检验得到的特征与标签的相关性，从笛卡尔积创建的特征当中筛选出 10 个特征
X_fsF = SelectKBest(f_regression, k=10).fit_transform(x_train[filter_columns], y_train)
```

In [119]:

```
X_fsF.shape
```

Out[119]:

```
(11999, 10)
```

In [127]:

```
# 把筛选出来的特征与原来的数据进行拼接
filtered_feature = pd.concat([x_train[origin_col], pd.DataFrame(X_fsF, index=x_train.index)], axis=1)
filtered_feature
```

In [128]:

```
x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)
```

In [129]:

```
random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(x_tr_f, y_tr_f)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va_f), y_va_f)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.4min finished
测试集上的MSE: 0.03167121172367826
```

In [131]:

```
random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)
grid_search_cv.fit(filtered_feature, y_train)


print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(filtered_feature), y_train)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.1min finished
测试集上的MSE: 0.004171659033699672
```

记录：

| 处理                             | MSE                  |
| -------------------------------- | -------------------- |
| 原始的one-hot                    | 0.03224754452495834  |
| 处理package                      | 0.031092557424066806 |
| 二阶笛卡尔积 + f检验筛出10个特征 | 0.03167121172367826  |

In [133]:

```
# f检验筛出 5 个特征
X_fsF = SelectKBest(f_regression, k=5).fit_transform(x_train[filter_columns], y_train)
filtered_feature = pd.concat([x_train[origin_col], pd.DataFrame(X_fsF, index=x_train.index)], axis=1)
print(filtered_feature.shape)
x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)
(11999, 40)
```

In [134]:

```
random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(x_tr_f, y_tr_f)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va_f), y_va_f)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.2min finished
测试集上的MSE: 0.03176599481181849
```

**此处发现筛选出5个特征比筛选出10个特征效果更差，尝试增加筛选的特征数**

In [156]:

```
selector = SelectKBest(f_regression, k=12)
filtered_array = selector.fit(x_train[filter_columns], y_train)
X_fsF = filtered_array.transform(x_train[filter_columns])
X_fsF.shape
```

Out[156]:

```
(11999, 12)
```

In [175]:

```
x = np.array(filter_columns)
x[filtered_array.get_support()]
```

Out[175]:

```
array(['number_project_6&time_spend_company_4',
       'number_project_6&Work_accident_0', 'number_project_6&package_1',
       'number_project_6&promotion_last_5years_0',
       'number_project_6&salary_1', 'time_spend_company_3&package_1',
       'time_spend_company_4&package_1', 'Work_accident_0&package_0',
       'Work_accident_0&package_1', 'package_0&promotion_last_5years_0',
       'package_1&promotion_last_5years_0', 'package_1&salary_1'],
      dtype='<U45')
```

In [172]:

```
# 验证 API get_support() 的正确性，取出对应列
last_col = X_fsF[:,-1]
last_col
```

Out[172]:

```
array([0, 1, 0, ..., 0, 1, 0], dtype=uint8)
```

In [177]:

```
# 验证 API get_support() 的正确性，取出对应列
b = x_train[filter_columns]['package_1&salary_1'].tolist()
b
```

In [179]:

```
last_col = last_col.tolist()
```

In [180]:

```
# 通过检验，证明 get_support() 得到的列名对应于 selectkbest 得到的列的顺序
for i in range(len(b)):
    if last_col[i] != b[i]:
        print(1)
```

In [181]:

```
# f 检验筛出 12 个特征
selector = SelectKBest(f_regression, k=12)
filtered_array = selector.fit(x_train[filter_columns], y_train)
"""
x = np.array(filter_columns)
x[filtered_array.get_support()]
"""
seleted_columns = filtered_array.get_support() # 筛选出来的特征的名字
X_fsF = filtered_array.transform(x_train[filter_columns])
```

In [182]:

```
filtered_feature = pd.concat([x_train[origin_col], pd.DataFrame(X_fsF, index=x_train.index)], axis=1)
print(filtered_feature.shape)
x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)

random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(x_tr_f, y_tr_f)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va_f), y_va_f)) ## 测试集上的MSE
(11999, 47)
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.6min finished
测试集上的MSE: 0.031699576694725985
```

In [186]:

```
x = np.array(filter_columns)
x_test[x[filtered_array.get_support()]]
```

In [187]:

```
merge_df = x_test[x[filtered_array.get_support()]]
x_test_temp = pd.concat([x_test[origin_col], merge_df], axis=1)
```

In [188]:

```
x_test_temp
```

记录：

| 处理                             | MSE                  |
| -------------------------------- | -------------------- |
| 原始的one-hot                    | 0.03224754452495834  |
| 处理package                      | 0.031092557424066806 |
| 二阶笛卡尔积 + f检验筛出10个特征 | 0.03167121172367826  |
| 二阶笛卡尔积 + f检验筛出12个特征 | 0.031554228990085434 |

In [189]:

```
random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(filtered_feature, y_train)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(filtered_feature), y_train)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.3min finished
测试集上的MSE: 0.00413405726456421
```

In [191]:

```
# 提交一哈

result = pd.DataFrame({'id': x_test_temp.index, 'satisfaction_level': grid_search_cv.predict(x_test_temp)})
time_stamp = get_local_time()
result.to_csv('./笛卡尔积+f检验_randomforset-%s.csv'%time_stamp, index=False)
```

**二阶笛卡尔积+ f 检验 MSE0.02933388**

### 互信息法特征筛选[¶](#互信息法特征筛选)

In [193]:

```
from sklearn.feature_selection import mutual_info_regression
```

In [194]:

```
# 互信息筛出 12 个特征
# WARNING---时间巨长
selector = SelectKBest(mutual_info_regression, k=12)
filtered_array = selector.fit(x_train[filter_columns], y_train)
"""
x = np.array(filter_columns)
x[filtered_array.get_support()]
"""
seleted_columns = filtered_array.get_support() # 筛选出来的特征的名字
X_fsF = filtered_array.transform(x_train[filter_columns])
```

In [195]:

```
filtered_feature = pd.concat([x_train[origin_col], pd.DataFrame(X_fsF, index=x_train.index)], axis=1)
print(filtered_feature.shape)
x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)

random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(x_tr_f, y_tr_f)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va_f), y_va_f)) ## 测试集上的MSE
(11999, 47)
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.6min finished
测试集上的MSE: 0.03172350497230676
```

In [196]:

```
x = np.array(filter_columns)
x_test[x[filtered_array.get_support()]]
merge_df = x_test[x[filtered_array.get_support()]]
x_test_temp = pd.concat([x_test[origin_col], merge_df], axis=1)

random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(filtered_feature, y_train)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(filtered_feature), y_train)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.1min finished
测试集上的MSE: 0.004165099380685169
```

In [198]:

```
# 提交一哈

result = pd.DataFrame({'id': x_test_temp.index, 'satisfaction_level': grid_search_cv.predict(x_test_temp)})
time_stamp = get_local_time()
result.to_csv('./笛卡尔积+互信息_randomforset-%s.csv'%time_stamp, index=False)
```

In [200]:

```
# 这块的数据就比较重要了，可以尝试直接用来跑 lightgbm，先把数据dump了
filtered_feature.to_pickle("./Dump/filtered_feature.pk")
x_test_temp.to_pickle("./Dump/x_test_temp.pk")
```

**效果拔群 MSE：0.02904271**

In [201]:

```
# 尝试一下用互信息法筛选更多特征

# 互信息筛出 25 个特征
# WARNING---时间巨长
selector = SelectKBest(mutual_info_regression, k=25)
filtered_array = selector.fit(x_train[filter_columns], y_train)
"""
x = np.array(filter_columns)
x[filtered_array.get_support()]
"""
seleted_columns = filtered_array.get_support() # 筛选出来的特征的名字
X_fsF = filtered_array.transform(x_train[filter_columns])
```

In [202]:

```
filtered_feature = pd.concat([x_train[origin_col], pd.DataFrame(X_fsF, index=x_train.index)], axis=1)
print(filtered_feature.shape)
x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)

random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(x_tr_f, y_tr_f)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va_f), y_va_f)) ## 测试集上的MSE
(11999, 60)
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.0min finished
测试集上的MSE: 0.03177106056150861
```

In [203]:

```
x = np.array(filter_columns)
x_test[x[filtered_array.get_support()]]
merge_df = x_test[x[filtered_array.get_support()]]
x_test_temp = pd.concat([x_test[origin_col], merge_df], axis=1)

random_forest_reg = RandomForestRegressor()
param_grid = {'n_estimators': [100,150,200,250]}
grid_search_cv = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(filtered_feature, y_train)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(filtered_feature), y_train)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.7min finished
测试集上的MSE: 0.004122380190191224
```

In [204]:

```
# 提交一哈

result = pd.DataFrame({'id': x_test_temp.index, 'satisfaction_level': grid_search_cv.predict(x_test_temp)})
time_stamp = get_local_time()
result.to_csv('./笛卡尔积+互信息25特征_randomforset-%s.csv'%time_stamp, index=False)
```

**MSE： 0.02922，此处可以尝试在25 和 12 之间取一个值，下次尝试 18**

**接下来的思路**

- 创建更多特征  
- 连续值特征处理、特征创建  
  - 连续值这块，因为要用树模型，可以尝试分箱处理，或者各种变换（标准化、minmaxscaler等）   
- 尝试单模 lgb + 调参  
- 模型融合  

## ExtraTreesRegressor[¶](#ExtraTreesRegressor)

In [213]:

```
from sklearn.ensemble import ExtraTreesRegressor

# 给 etr feed 互信息k=12
filtered_feature = pd.concat([x_train[origin_col], pd.DataFrame(X_fsF, index=x_train.index)], axis=1)
print(filtered_feature.shape)
x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)

etr = ExtraTreesRegressor()
param_grid = {'n_estimators': [60,100,150, 200]}
grid_search_cv = GridSearchCV(estimator=etr, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(x_tr_f, y_tr_f)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va_f), y_va_f)) ## 测试集上的MSE
(11999, 60)
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.0min finished
测试集上的MSE: 0.035421482522685184
```

In [214]:

```
x = np.array(filter_columns)
x_test[x[filtered_array.get_support()]]
merge_df = x_test[x[filtered_array.get_support()]]
x_test_temp = pd.concat([x_test[origin_col], merge_df], axis=1)

etr = ExtraTreesRegressor()
param_grid = {'n_estimators': [60,100,150, 200]}
grid_search_cv = GridSearchCV(estimator=etr, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(filtered_feature, y_train)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(filtered_feature), y_train)) ## 测试集上的MSE
Fitting 5 folds for each of 5 candidates, totalling 25 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  3.5min finished
测试集上的MSE: 6.875572964413688e-09
```

In [217]:

```
# 提交一哈

result = pd.DataFrame({'id': x_test_temp.index, 'satisfaction_level': grid_search_cv.predict(x_test_temp)})
time_stamp = get_local_time()
result.to_csv('./笛卡尔积+互信息25特征_ExtraTreesRegressor-%s.csv'%time_stamp, index=False)
```

In [216]:

```
# 把互信息筛选25维的训练集合测试集dump
filtered_feature.to_pickle("./Dump/filtered_feature25.pk")
x_test_temp.to_pickle("./Dump/x_test_temp25.pk")
```

In [218]:

```
# 尝试用 互信息 k=12 训练 etr
filtered_feature = pd.read_pickle("./Dump/filtered_feature.pk")
x_test_temp = pd.read_pickle("./Dump/x_test_temp.pk")
```

In [219]:

```
x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)

etr = ExtraTreesRegressor()
param_grid = {'n_estimators': [60,100,150, 200]}
grid_search_cv = GridSearchCV(estimator=etr, param_grid=param_grid, 
                              scoring=make_scorer(mean_squared_error), n_jobs=-1, pre_dispatch=1, cv=5, verbose=1)

grid_search_cv.fit(x_tr_f, y_tr_f)

print('测试集上的MSE:',mean_squared_error(grid_search_cv.predict(x_va_f), y_va_f)) ## 测试集上的MSE
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.7min finished
测试集上的MSE: 0.03565125116481481
```

## 尝试 LightGBM[¶](#尝试-LightGBM)

In [220]:

```
import lightgbm as lgb

x_tr_f, x_va_f, y_tr_f, y_va_f = train_test_split(filtered_feature, y_train, test_size = 0.2, random_state=30)
train_X = x_tr_f
test_X = x_va_f

# 5.转换为Dataset数据格式
lgb_train = lgb.Dataset(train_X, y_tr_f)
lgb_eval = lgb.Dataset(test_X, y_va_f, reference=lgb_train)

# 6.参数
params = {
    'task': 'train',
    'boosting_type': 'gbdt',  # 设置提升类型
    'objective': 'regression',  # 目标函数
    'metric': {'l1', 'mse'},  # 评估函数
    'num_leaves': 20,  # 叶子节点数
    'learning_rate': 0.05,  # 学习速率
    'feature_fraction': 0.8,  # 建树的特征选择比例
    'bagging_fraction': 0.7,  # 建树的样本采样比例
    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging
    'verbose': 1  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息
}


# 7.调用LightGBM模型，使用训练集数据进行训练（拟合）
# Add verbosity=2 to print messages while running boosting
my_model = lgb.train(params, lgb_train, num_boost_round=100, valid_sets=lgb_eval, early_stopping_rounds=10)

# 8.使用模型对测试集数据进行预测
predictions = my_model.predict(test_X, num_iteration=my_model.best_iteration)

# 9.对模型的预测结果进行评判（平均绝对误差）
print("MSE : " + str(mean_squared_error(predictions, y_va_f)))
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001133 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 411
[LightGBM] [Info] Number of data points in the train set: 9599, number of used features: 47
[LightGBM] [Info] Start training from score 0.614364
[1]	valid_0's l1: 0.198765	valid_0's l2: 0.0571681
Training until validation scores don't improve for 10 rounds
[2]	valid_0's l1: 0.194829	valid_0's l2: 0.0547906
[3]	valid_0's l1: 0.191061	valid_0's l2: 0.0525966
[4]	valid_0's l1: 0.187561	valid_0's l2: 0.0506119
[5]	valid_0's l1: 0.18433	valid_0's l2: 0.0488462
[6]	valid_0's l1: 0.181382	valid_0's l2: 0.0472986
[7]	valid_0's l1: 0.178625	valid_0's l2: 0.0459199
[8]	valid_0's l1: 0.175974	valid_0's l2: 0.0446636
[9]	valid_0's l1: 0.173456	valid_0's l2: 0.0434915
[10]	valid_0's l1: 0.171096	valid_0's l2: 0.0424373
[11]	valid_0's l1: 0.168888	valid_0's l2: 0.0414883
[12]	valid_0's l1: 0.166784	valid_0's l2: 0.0406075
[13]	valid_0's l1: 0.164795	valid_0's l2: 0.0398002
[14]	valid_0's l1: 0.162857	valid_0's l2: 0.0390651
[15]	valid_0's l1: 0.161082	valid_0's l2: 0.0384172
[16]	valid_0's l1: 0.159465	valid_0's l2: 0.0378428
[17]	valid_0's l1: 0.157979	valid_0's l2: 0.0373417
[18]	valid_0's l1: 0.156572	valid_0's l2: 0.0368898
[19]	valid_0's l1: 0.155178	valid_0's l2: 0.0364611
[20]	valid_0's l1: 0.153941	valid_0's l2: 0.0360917
[21]	valid_0's l1: 0.152722	valid_0's l2: 0.0357436
[22]	valid_0's l1: 0.151561	valid_0's l2: 0.035429
[23]	valid_0's l1: 0.150568	valid_0's l2: 0.0351793
[24]	valid_0's l1: 0.149657	valid_0's l2: 0.0349555
[25]	valid_0's l1: 0.1487	valid_0's l2: 0.0347342
[26]	valid_0's l1: 0.147904	valid_0's l2: 0.0345544
[27]	valid_0's l1: 0.14705	valid_0's l2: 0.0343592
[28]	valid_0's l1: 0.146312	valid_0's l2: 0.0341949
[29]	valid_0's l1: 0.145637	valid_0's l2: 0.0340418
[30]	valid_0's l1: 0.145039	valid_0's l2: 0.033931
[31]	valid_0's l1: 0.144414	valid_0's l2: 0.0338029
[32]	valid_0's l1: 0.143869	valid_0's l2: 0.0337044
[33]	valid_0's l1: 0.143333	valid_0's l2: 0.0335989
[34]	valid_0's l1: 0.142898	valid_0's l2: 0.0335231
[35]	valid_0's l1: 0.142454	valid_0's l2: 0.0334322
[36]	valid_0's l1: 0.14206	valid_0's l2: 0.0333566
[37]	valid_0's l1: 0.141668	valid_0's l2: 0.0332768
[38]	valid_0's l1: 0.141301	valid_0's l2: 0.0332038
[39]	valid_0's l1: 0.140963	valid_0's l2: 0.0331487
[40]	valid_0's l1: 0.140623	valid_0's l2: 0.0330809
[41]	valid_0's l1: 0.140299	valid_0's l2: 0.0330207
[42]	valid_0's l1: 0.14003	valid_0's l2: 0.0329646
[43]	valid_0's l1: 0.139737	valid_0's l2: 0.0329053
[44]	valid_0's l1: 0.13952	valid_0's l2: 0.0328728
[45]	valid_0's l1: 0.139353	valid_0's l2: 0.0328351
[46]	valid_0's l1: 0.139143	valid_0's l2: 0.0328141
[47]	valid_0's l1: 0.138957	valid_0's l2: 0.0327814
[48]	valid_0's l1: 0.138762	valid_0's l2: 0.0327557
[49]	valid_0's l1: 0.138545	valid_0's l2: 0.0327237
[50]	valid_0's l1: 0.13837	valid_0's l2: 0.0327027
[51]	valid_0's l1: 0.138228	valid_0's l2: 0.032698
[52]	valid_0's l1: 0.138062	valid_0's l2: 0.0326843
[53]	valid_0's l1: 0.137917	valid_0's l2: 0.0326667
[54]	valid_0's l1: 0.137768	valid_0's l2: 0.0326476
[55]	valid_0's l1: 0.137651	valid_0's l2: 0.0326355
[56]	valid_0's l1: 0.137543	valid_0's l2: 0.0326211
[57]	valid_0's l1: 0.137474	valid_0's l2: 0.0326191
[58]	valid_0's l1: 0.137395	valid_0's l2: 0.0326123
[59]	valid_0's l1: 0.137342	valid_0's l2: 0.0326167
[60]	valid_0's l1: 0.137238	valid_0's l2: 0.0326148
[61]	valid_0's l1: 0.137174	valid_0's l2: 0.0326212
[62]	valid_0's l1: 0.137099	valid_0's l2: 0.0326235
[63]	valid_0's l1: 0.13704	valid_0's l2: 0.0326366
[64]	valid_0's l1: 0.136931	valid_0's l2: 0.0326203
[65]	valid_0's l1: 0.136887	valid_0's l2: 0.0326077
[66]	valid_0's l1: 0.136791	valid_0's l2: 0.0326041
[67]	valid_0's l1: 0.1367	valid_0's l2: 0.0325873
[68]	valid_0's l1: 0.136597	valid_0's l2: 0.0325721
[69]	valid_0's l1: 0.136498	valid_0's l2: 0.0325486
[70]	valid_0's l1: 0.136458	valid_0's l2: 0.0325534
[71]	valid_0's l1: 0.13639	valid_0's l2: 0.0325399
[72]	valid_0's l1: 0.13636	valid_0's l2: 0.0325383
[73]	valid_0's l1: 0.13632	valid_0's l2: 0.032529
[74]	valid_0's l1: 0.1363	valid_0's l2: 0.0325269
[75]	valid_0's l1: 0.136273	valid_0's l2: 0.032519
[76]	valid_0's l1: 0.136265	valid_0's l2: 0.0325229
[77]	valid_0's l1: 0.136259	valid_0's l2: 0.0325183
[78]	valid_0's l1: 0.136255	valid_0's l2: 0.0325163
[79]	valid_0's l1: 0.136208	valid_0's l2: 0.0325039
[80]	valid_0's l1: 0.136211	valid_0's l2: 0.0325095
[81]	valid_0's l1: 0.136195	valid_0's l2: 0.0325175
[82]	valid_0's l1: 0.136186	valid_0's l2: 0.0325249
[83]	valid_0's l1: 0.13619	valid_0's l2: 0.0325264
[84]	valid_0's l1: 0.136212	valid_0's l2: 0.0325343
[85]	valid_0's l1: 0.136194	valid_0's l2: 0.0325314
[86]	valid_0's l1: 0.136136	valid_0's l2: 0.0325128
[87]	valid_0's l1: 0.136082	valid_0's l2: 0.0324997
[88]	valid_0's l1: 0.136062	valid_0's l2: 0.0324955
[89]	valid_0's l1: 0.136042	valid_0's l2: 0.0324942
[90]	valid_0's l1: 0.136061	valid_0's l2: 0.0324973
[91]	valid_0's l1: 0.136046	valid_0's l2: 0.0324903
[92]	valid_0's l1: 0.136037	valid_0's l2: 0.0324908
[93]	valid_0's l1: 0.13605	valid_0's l2: 0.0324938
[94]	valid_0's l1: 0.136023	valid_0's l2: 0.0324749
[95]	valid_0's l1: 0.136011	valid_0's l2: 0.032465
[96]	valid_0's l1: 0.136006	valid_0's l2: 0.0324662
[97]	valid_0's l1: 0.136002	valid_0's l2: 0.0324723
[98]	valid_0's l1: 0.136015	valid_0's l2: 0.0324827
[99]	valid_0's l1: 0.136027	valid_0's l2: 0.0324787
[100]	valid_0's l1: 0.136028	valid_0's l2: 0.0324899
Did not meet early stopping. Best iteration is:
[97]	valid_0's l1: 0.136002	valid_0's l2: 0.0324723
MSE : 0.03247225323077779
```

In [221]:

```
params = {
    'task': 'train',
    'boosting_type': 'gbdt',  # 设置提升类型
    'objective': 'regression',  # 目标函数
    'metric': 'mse',  # 评估函数
    'learning_rate': 0.1,
    'max_depth': 5,     # 由于数据集不是很大，所以选择了一个适中的值，其实4-10都无所谓。
    'num_leaves': 30,   # 由于lightGBM是leaves_wise生长，官方说法是要小于2^max_depth
    'subsample':0.8,           # 数据采样
    'colsample_bytree': 0.8   # 特征采样
}

lgb_train = lgb.Dataset(x_tr_f, y_tr_f)
cv_results = lgb.cv(params, lgb_train, num_boost_round=1000, nfold=10, stratified=False, shuffle=True, metrics='mse',early_stopping_rounds=50,seed=0)
```

In [222]:

```
print('best n_estimators:', len(cv_results['l2-mean']))
best n_estimators: 113
```

In [223]:

```
min(cv_results["l2-mean"])
```

Out[223]:

```
0.03216117347733203
```

**lightgbm 练死劲儿不好用，咱直接从当前最好的方案出发，进行特征创建**<br>
<br>
**接下来的思路**

- 创建更多特征  
- 连续值特征处理、特征创建  
  - 连续值这块，因为要用树模型，可以尝试分箱处理，或者各种变换（标准化、minmaxscaler等）   
- 尝试单模 lgb + 调参  
- 模型融合  

In [ ]:

```

```